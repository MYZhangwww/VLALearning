# 14. Training: Img-Txt Align -> Visual Instruction Tuning

VLM 的训练范式经历了从“对齐”到“指令微调”的巨大转变。这一转变让 VLM 从一个只会分类或简短描述的工具，变成了真正的**多模态聊天机器人 (Visual Chatbot)**。

## 1. 阶段一：图文对齐 (Image-Text Alignment)
以 **CLIP (OpenAI)** 为代表。
*   **数据**：海量的 `(Image, Text)` 对（如 LAION-400M）。
*   **目标**：**对比学习 (Contrastive Learning)**。拉近匹配的图文对的特征距离，推远不匹配的。
*   **能力**：极强的 Zero-Shot 分类和检索能力。
*   **局限**：**无法生成连续文本**。它只能告诉你图片和那句话“像不像”，不能回答“图片里有什么”。

## 2. 阶段二：生成式预训练 (Captioning)
以 **BLIP / SimVLM** 为代表。
*   **数据**：还是图文对，但质量更高（如 COCO Captions）。
*   **目标**：**Language Modeling (LM)**。给定图片，预测对应的文本描述。
*   **能力**：Image Captioning（看图说话）、VQA（简短问答）。
*   **局限**：**只会陈述事实，不懂指令**。如果你问它“这张图片好笑在哪里？”，它可能只会回答“一只猫在跳舞”，而无法解释笑点。

## 3. 阶段三：视觉指令微调 (Visual Instruction Tuning)
**LLaVA (Large Language and Vision Assistant)** 的出现定义了现代 VLM 的训练范式。
核心思想：**将视觉任务转化为多轮对话格式**。

### (a) 数据构造 (The Magic of GPT-4)
LLaVA 没有大量人工标注的对话数据。它利用纯文本的 GPT-4：
1.  输入图片的详细描述（Captions）和物体边框（Bounding Boxes）。
2.  让 GPT-4 **脑补**出一段关于这张图的对话。
    *   *User*: "这张图里有什么异常吗？"
    *   *Assistant*: "异常的是这只猫像人一样站着..."
3.  这就生成了 158k 条高质量的**多模态对话数据**。

### (b) 训练过程 (SFT)
将 VLM 在这些生成的数据上进行 **Supervised Fine-Tuning (SFT)**。
*   **输入**：`<Image> User: ... \n Assistant:`
*   **输出**：预测 Assistant 的回答。

**结果**：模型不仅学会了看图，更学会了**遵从人类指令 (Follow Instructions)**，能够进行复杂的推理、解释代码、编写故事。

## 4. 为什么"看图聊天"主要在 SFT 阶段训练？

这是一个很关键的问题。答案是：**每个训练阶段各有分工，看图聊天能力必须等前置阶段打好基础后，才能在 SFT 阶段被"激活"。**

### (a) 各阶段为什么不训练"看图聊天"？

| 阶段 | 为什么不在这里训练看图聊天 |
| :--- | :--- |
| **Stage 0: LLM 预训练** | 此时模型根本没接过视觉信号，只有纯文本。它的任务是学会语言能力（语法、知识、推理）。这是"大脑"的基础建设，和"眼睛"无关。 |
| **Stage 1: 图文对齐 (CLIP 式 / Projector 预训练)** | 此阶段的目标是**让视觉 token 和文本 token 住进同一个语义空间**。通常使用简短的 `(Image, Caption)` 对，只训练 Connector/Projector，LLM 权重冻结。如果此时就用复杂对话数据训练，Projector 还没学会基本的"翻译"（把像素变成 LLM 能懂的 token），模型会在"既不懂图、又要聊天"的双重困难中崩溃，**两个目标互相干扰，什么都学不好**。 |
| **Stage 2: RLHF / DPO** | 这是 SFT **之后**的精修阶段。它不教新能力，而是**校准已有能力的偏好**——减少幻觉、让回答更符合人类期望。如果模型连看图聊天都还不会，RLHF 没有东西可以"校准"。 |

### (b) 为什么 SFT 是最合适的阶段？

核心原因有三：

1.  **前置条件已就绪**
    *   LLM 已具备语言理解和生成能力（来自预训练）。
    *   Projector 已学会把图像特征"翻译"成 LLM 能理解的 token（来自对齐阶段）。
    *   SFT 阶段只需要把这两个能力**桥接**起来：让模型学会"根据图像内容 + 用户指令，生成符合对话格式的回答"。

2.  **对话格式需要显式监督**
    *   "看图聊天"本质上是一种**格式化输出**能力——模型需要学会：何时该详细描述、何时该简短回答、何时该推理、何时该拒绝回答。
    *   这种格式化能力**只能通过有监督数据 (SFT) 来教**，靠无监督预训练或对比学习都学不会。

3.  **数据效率最高**
    *   SFT 数据量相比预训练极少（几十万条 vs 几十亿条），但格式明确、质量高（如 LLaVA 的 GPT-4 生成数据）。
    *   在 SFT 阶段，模型的"基础能力"已经存在，只需少量高质量对话数据就能"点亮"看图聊天技能，类似于一个会外语的人只需要少量对话练习就能流利会话。

### (c) 类比理解：学画画解说

把整个过程想象成培养一个**体育解说员**：
*   **LLM 预训练** → 先学会说话、掌握语言
*   **图文对齐** → 让他学会"看"比赛画面，建立视觉到语言的映射
*   **SFT (指令微调)** → 给他大量优秀解说录像作为示范，教他"看到什么画面该说什么话" ← **看图聊天能力在这里形成**
*   **RLHF** → 让观众打分，微调他的解说风格，减少错误

> **一句话总结**：SFT 阶段是"看图聊天"能力形成的最佳时机——因为此时"看"的能力和"说"的能力都已就绪，SFT 只需通过高质量对话数据把两者串联起来。

## 5. 阶段四：VLM 的 RLHF

### (a) 什么是 RLHF？

**RLHF (Reinforcement Learning from Human Feedback)**，即**基于人类反馈的强化学习**，是在 SFT 之后的一个精修阶段。它的核心思想是：**用人类的偏好判断来进一步优化模型的输出质量**。

> 类比：SFT 像是给学生一本标准教材让他照着学；RLHF 像是请一位老师坐在旁边，对学生的每次回答打分——"这个好"、"那个不好"——让学生逐渐学会什么样的回答更受欢迎。

### (b) RLHF 的主要作用

RLHF 的作用**不是教模型新能力**，而是**校准和精修已有能力**。具体来说：

| 作用 | 说明 | 举例 |
| :--- | :--- | :--- |
| **减少幻觉 (Reduce Hallucination)** | 模型在 SFT 后可能会"编造"图片中不存在的内容。RLHF 通过惩罚这类错误输出，让模型学会"不确定就不说"。 | 图片里只有一只猫，模型却说"两只猫在玩耍" → RLHF 后模型会更忠实于图片内容 |
| **提升有用性 (Helpfulness)** | 让回答更详细、更切题、更有信息量，而非敷衍了事。 | 用户问"这道菜怎么做"，模型不再只说"这是一道菜"，而是给出具体步骤 |
| **增强安全性 (Safety)** | 让模型拒绝有害请求、避免生成不当内容。 | 用户上传不当图片要求描述 → 模型学会礼貌拒绝 |
| **对齐人类偏好 (Alignment)** | 让模型的表达风格、详略程度、语气等更符合人类期望。 | 同样的内容，RLHF 后模型会选择更清晰、更有条理的表达方式 |

### (c) RLHF 的标准流程（三步法）

```
Step 1: SFT 模型 (已完成)
   ↓
Step 2: 训练奖励模型 (Reward Model, RM)
   ↓
Step 3: 用 RL (PPO) 优化策略模型
```

**Step 1 — SFT 模型（前置条件）**
经过 SFT 后的模型已经能"看图聊天"，但质量参差不齐。

**Step 2 — 训练奖励模型 (Reward Model)**
1.  给同一个输入（图片 + 问题），让 SFT 模型生成**多个不同回答**。
2.  让**人类标注员对这些回答排序**：回答 A > 回答 B > 回答 C。
3.  用这些排序数据训练一个**奖励模型 (RM)**，使它能自动给任意回答打分。

**Step 3 — PPO 强化学习优化**
1.  模型对新输入生成回答。
2.  奖励模型对回答打分。
3.  用 **PPO (Proximal Policy Optimization)** 算法更新模型参数，让模型倾向于生成高分回答。
4.  同时加入 **KL 散度约束**，防止模型为了讨好 RM 而偏离 SFT 阶段学到的基础能力（即防止"reward hacking"）。

### (d) DPO：RLHF 的简化替代方案

**DPO (Direct Preference Optimization)** 是 RLHF 的一种更高效的替代：
*   **省去了训练奖励模型的步骤**，直接用偏好数据优化策略模型。
*   数学上等价于 RLHF，但实现更简单、训练更稳定。
*   在 VLM 领域被广泛采用（如 Silkie, LLaVA-RLHF 的后续工作）。

### (e) VLM 中 RLHF 的代表工作

最新的研究（如 **LLaVA-RLHF**, **Silkie**）开始将 RLHF / DPO 引入 VLM：
*   **LLaVA-RLHF**：首次将 RLHF 应用于多模态模型，显著减少了视觉幻觉。
*   **Silkie**：使用 GPT-4V 自动生成偏好数据（代替人工标注），再用 DPO 训练，进一步降低了数据成本。
*   **核心目标**：减少幻觉（Hallucination），让模型不描述图片里不存在的物体，更诚实 (Honest) 和有帮助 (Helpful)。

### (f) 一句话总结 RLHF

> **RLHF 不教模型"做什么"，而是教模型"怎么做得更好"——它是 SFT 之后的品质管控环节，通过人类偏好信号让模型的输出从"能用"升级为"好用"。**

## 6. 总结

| 特性 | CLIP (对齐) | LLaVA (指令微调) |
| :--- | :--- | :--- |
| **核心逻辑** | `Match(Img, Txt)` | `Chat(Img, User_Instruction)` |
| **数据来源** | 爬虫 (LAION) | GPT-4 生成 / 人工改写 |
| **能力边界** | 分类, 检索 | **复杂对话, 推理, 代码** |
| **训练目标** | 对比损失 (InfoNCE) | 交叉熵 (Next Token Prediction) |
| **地位** | 视觉编码器的基石 | **多模态助手的基石** |
