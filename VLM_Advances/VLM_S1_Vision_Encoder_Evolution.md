# S1. 视觉编码器演进：CLIP → SigLIP → DINOv2

文档 11 介绍了 **Connector（连接器）** 的演进，但 VLM 的另一半核心——**视觉编码器 (Vision Encoder)** 本身的演进同样至关重要。本篇聚焦于视觉编码器从 CLIP 到 SigLIP 2 的发展历程。

## 1. 为什么视觉编码器如此重要？

在 VLM 架构中，视觉编码器是"眼睛"——它决定了模型能从图像中**看到什么、看到多少**。

```
[RGB 图像] → 📷 视觉编码器 → [视觉 Token 序列] → Connector → LLM
                   ↑
            如果这里看漏了信息
            后面再怎么强也补不回来
```

> **类比**：LLM 是大脑，Connector 是翻译，视觉编码器是眼睛。一个近视的眼睛（弱编码器）配上天才的大脑，也看不清远处的字。

## 2. 演进时间线

```
2021.01  CLIP (OpenAI)          ← 开创性的图文对比学习
2023.03  SigLIP (Google)        ← Sigmoid 替代 Softmax，更好的扩展性
2023.04  DINOv2 (Meta)          ← 自监督学习，空间感知之王
2023.12  InternViT-6B (上海 AI Lab) ← 6B 参数的巨型视觉编码器
2024.02  Prismatic VLMs         ← 🔑 发现"双编码器融合"效果最佳
2025.02  SigLIP 2 (Google)      ← 多语言 + 多目标训练，最新 SOTA
```

## 3. 各代编码器详解

### (a) CLIP ViT（2021，OpenAI）

**训练方式**：对比学习 (Contrastive Learning)
```
正样本：(猫的图片, "a photo of a cat")    → 拉近距离
负样本：(猫的图片, "a red sports car")     → 推远距离
```

**核心架构**：ViT-L/14（307M 参数），输入 224x224 或 336x336。

**优点**：
*   极强的**语义理解**：知道"这是猫"、"这是车"。
*   优秀的 Zero-Shot 分类和检索能力。
*   在海量互联网图文对 (400M+) 上训练，知识面极广。

**缺点**：
*   **空间感知弱**：知道"图里有猫"，但不确定猫在哪里。
*   **细节丢失**：对比学习强调全局语义匹配，忽略了局部细节（小文字、纹理）。
*   **分辨率固定**：预训练在 224/336，大图必须 Resize。

**在 VLM 中的地位**：2021-2023 年几乎所有 VLM（LLaVA, BLIP-2, InstructBLIP）都使用 CLIP ViT-L 作为视觉编码器，它是 VLM 时代的"标配眼睛"。

### (b) SigLIP（2023，Google）

**改进点**：将 CLIP 的 **Softmax 对比损失** 替换为 **Sigmoid 对比损失**。

```
CLIP Loss:   Softmax over entire batch → 需要全 batch 的 all-pairs 计算
SigLIP Loss: Sigmoid per image-text pair → 每对独立计算
```

**为什么这个改变重要？**

| 特性 | CLIP (Softmax) | SigLIP (Sigmoid) |
| :--- | :--- | :--- |
| 损失函数 | 全 batch 的交叉熵 | 每对独立的二元交叉熵 |
| Batch Size 依赖 | 强依赖（越大越好） | 弱依赖（更灵活） |
| 分布式训练 | 需要 all-gather 同步 | 更容易分布式扩展 |
| 性能 | 基线 | 在同等设置下**更好** |

**在 VLM 中的应用**：OpenVLA 等模型开始用 SigLIP 替代 CLIP 作为语义编码器。

### (c) DINOv2（2023，Meta）

**训练方式**：自监督学习 (Self-Supervised Learning)——**不使用任何文本标签**。

```
DINOv2 训练流程：
  原图 → [随机裁剪/增广] → 得到两个视角
     ↓                        ↓
  Teacher 编码              Student 编码
     ↓                        ↓
  Teacher 特征    ←自蒸馏→   Student 特征
                   +
               掩码预测 (MAE 风格)
```

**核心优势**：**极强的空间感知和局部细节表达**。

| 特性 | CLIP/SigLIP | DINOv2 |
| :--- | :--- | :--- |
| **训练信号** | 图文对（语义级别） | 图像自身（像素级别） |
| **语义理解** | ✅ 强 | 中等 |
| **空间感知** | 弱（全局匹配） | **✅ 极强（局部特征）** |
| **深度/法线估计** | 弱 | **✅ 极强** |
| **分割能力** | 需要微调 | **✅ 几乎零样本** |
| **适合任务** | 分类、检索、VQA | **定位、分割、空间推理** |

**在 VLA 中的关键作用**：DINOv2 的空间感知能力对机器人操作至关重要——机器人需要知道"杯子在桌面的哪个精确位置"，而不仅仅是"画面中有杯子"。

### (d) 双编码器融合：DINOv2 + SigLIP（2024，Prismatic VLMs）

**核心发现**：Stanford 的 Prismatic VLMs 研究系统探索了 VLM 的视觉编码器设计空间，得出了一个关键结论：

> **"将 DINOv2 和 SigLIP 的特征拼接在一起，效果超过任何单一编码器。"**

```
[RGB 图像]
    ↓               ↓
┌─────────┐    ┌─────────┐
│ DINOv2  │    │ SigLIP  │
│ 空间感知 │    │ 语义理解 │
└────┬────┘    └────┬────┘
     │              │
     └── 拼接 (Concat) ──→ [融合视觉 Token] → Connector → LLM
```

**为什么互补？**
*   SigLIP 说："画面里有一个杯子和一个盘子"（语义）
*   DINOv2 说："坐标 (0.3, 0.7) 处有一个圆形物体，坐标 (0.6, 0.4) 处有一个扁平物体"（空间）
*   两者结合：LLM 知道"杯子在左下角，盘子在右上角"

**影响**：双编码器成为了后续模型（如 OpenVLA）的标准配置。

### (e) SigLIP 2（2025.02，Google）

**最新进展**：Google 发布的 SigLIP 2 代表了当前视觉编码器的最高水平。

**关键创新**：
1.  **多目标训练**：同时使用对比学习 + Caption 生成 + 自蒸馏 + 掩码预测。
2.  **多语言支持**：支持多种语言的图文对齐。
3.  **多分辨率 + 原生长宽比**：不再强制正方形输入。
4.  **在线数据筛选**：训练过程中动态过滤低质量数据。

**模型规模**：ViT-B (86M) / ViT-L (303M) / So400m (400M) / ViT-g (1B)。

**关键发现**：

> **"训练方法比模型规模更重要"——400M 的 SigLIP 2 在大多数 VLM 基准上超过了 6B 的 InternViT-6B。**

**应用**：已被 Qwen3-VL、Gemma 3 等最新 VLM 采用。

## 4. 全面对比

| 编码器 | 参数量 | 训练方式 | 语义 | 空间 | 分辨率 | 典型用途 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| CLIP ViT-L | 307M | 对比学习(图文) | ⭐⭐⭐⭐ | ⭐⭐ | 336 固定 | LLaVA, BLIP-2 |
| SigLIP So400m | 400M | Sigmoid 对比 | ⭐⭐⭐⭐⭐ | ⭐⭐ | 多分辨率 | OpenVLA |
| DINOv2 ViT-L | 307M | 自监督(纯图) | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 多分辨率 | OpenVLA (空间) |
| InternViT-6B | 6B | 对比+生成 | ⭐⭐⭐⭐ | ⭐⭐⭐ | 动态 | InternVL |
| **SigLIP 2 So400m** | 400M | **多目标混合** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | **原生长宽比** | **Qwen3-VL, Gemma 3** |

## 5. 关键洞察总结

1.  **训练方法 > 模型规模**：SigLIP 2 (400M) 打败 InternViT (6B)，说明好的训练配方比堆参数更重要。
2.  **双编码器是过渡方案**：DINOv2 + SigLIP 双编码器虽然效果好，但 SigLIP 2 通过多目标训练将语义和空间能力统一到了单一编码器中，可能使双编码器不再必要。
3.  **编码器越来越"全能"**：从单一目标（对比 or 自监督）→ 多目标联合训练，编码器同时具备语义 + 空间 + 多语言能力。

> **一句话总结**：视觉编码器从 CLIP 的"知道是什么"→ DINOv2 的"知道在哪里"→ SigLIP 2 的"既知道是什么又知道在哪里"，这条演进路线决定了 VLM 从"看大概"到"看精确"的能力跃迁。
