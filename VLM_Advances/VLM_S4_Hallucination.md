# S4. 多模态幻觉：VLM 最致命的缺陷

## 1. 什么是多模态幻觉？

**多模态幻觉 (Multimodal Hallucination)** 是 VLM 特有的严重问题：**模型描述了图片中根本不存在的内容**。

```
[图片：一只猫坐在沙发上]

用户: "请描述这张图片。"
VLM:  "一只猫和一只狗坐在沙发上，旁边有一个花瓶。"
                  ↑ 不存在           ↑ 不存在
                  幻觉!              幻觉!
```

> **为什么比 LLM 幻觉更危险？** LLM 幻觉是"说错知识"（可以查证），VLM 幻觉是"看见了不存在的东西"（直接误导用户）。在医学影像、自动驾驶、机器人操作中，这可能是致命的。

## 2. 幻觉的类型

| 类型 | 说明 | 举例 |
| :--- | :--- | :--- |
| **物体幻觉** | 描述了不存在的物体 | 图里只有猫，说"有猫和狗" |
| **属性幻觉** | 物体存在但属性错误 | 红色杯子说成"蓝色杯子" |
| **关系幻觉** | 物体间的关系错误 | 猫在桌下，说"猫在桌上" |
| **数量幻觉** | 物体数量错误 | 两只猫说成"三只猫" |
| **动作幻觉** | 描述了未发生的动作 | 猫在睡觉，说"猫在跳舞" |

其中**物体幻觉**是最常见也最被研究的类型。

## 3. 幻觉从哪里来？

### (a) LLM 的语言先验太强

LLM 在预训练中学到了"常识性共现"——比如"厨房里通常有冰箱、微波炉、水槽"。当 VLM 看到一张厨房图片时，LLM 倾向于**根据语言先验"脑补"**，而不是忠实地只描述看到的内容。

```
视觉信号:    [厨房，有灶台和水槽]
LLM 先验:    "厨房通常有冰箱"
输出:         "厨房里有灶台、水槽和冰箱"  ← 冰箱是脑补的！
```

### (b) 视觉信息不足

如果视觉编码器的输出模糊或有损（低分辨率、Connector 压缩过度），LLM 会用语言先验"填补"视觉信息的空白。

### (c) 训练数据的偏差

训练数据中的 Caption 本身可能含有错误或遗漏，模型学到了"不完全忠于图片"的习惯。

### (d) 生成过程中的雪球效应

自回归生成时，一旦产生了一个幻觉 Token，后续 Token 会基于这个错误继续"自圆其说"。

```
正常:  "图中有一只猫"
幻觉:  "图中有一只猫和一只" → LLM 自然续写 → "狗" → "它们在一起玩耍" ← 全是幻觉
```

## 4. 评测基准

### (a) POPE (Polling-based Object Probing Evaluation)

最经典的物体幻觉评测：

```
方法：对每张图片提问 "Is there a [object] in the image?"
                                        ↓
                          模型回答 Yes/No
                                        ↓
                      与 Ground Truth 对比，计算 F1
```

三种难度：
*   **Random**：随机选择不存在的物体提问。
*   **Popular**：选择数据集中最常见的物体提问（更容易被语言先验误导）。
*   **Adversarial**：选择与图中已有物体经常共现的物体提问（最难，如图中有"热狗"就问"有没有面包"）。

### (b) CHAIR (Caption Hallucination Assessment with Image Relevance)

评估 Caption 生成中的幻觉比例：

```
CHAIR_i = (包含幻觉物体的句子数) / (总句子数)
CHAIR_s = (幻觉物体数) / (生成的总物体数)
```

越低越好。

### (c) MMHal-Bench

更全面的幻觉评测，涵盖物体、属性、关系、数量等多种幻觉类型。

## 5. 减少幻觉的方法

### (a) 训练时方法

#### RLHF / DPO for VLM

在第 14 篇中已详细介绍。用人类偏好数据训练模型"不幻觉"。

```
好的回答（忠于图片）: 高奖励 ✅
幻觉回答（编造内容）: 低奖励 ❌
→ 模型学会只描述看到的内容
```

**代表**：LLaVA-RLHF, Silkie, V-DPO。

#### 高质量训练数据

*   使用更准确的 Caption 数据（人工标注 > GPT-4 生成 > 自动生成）。
*   对训练数据做**去偏处理**——减少"厨房必有冰箱"这类统计偏差。

### (b) 推理时方法（无需重新训练）

#### 对比解码 (Contrastive Decoding)

**核心思想**：同时运行两个分支——正常分支和"受干扰"分支，用两者的差异过滤掉幻觉。

```
正常输入:     [真实图片] + "描述图片" → P_normal(token)
干扰输入:     [噪声图片] + "描述图片" → P_distorted(token)
                                          ↓
最终输出:     P_final = P_normal - α × P_distorted
                        ↑ 减去"不看图也会说的部分" = 减去语言先验 = 减少幻觉
```

**代表方法**：

| 方法 | 干扰方式 | 效果 | 额外训练 |
| :--- | :--- | :--- | :--- |
| **VCD** | 添加视觉噪声 | POPE +3-5% | ❌ |
| **ICD** | 修改指令（加入误导信息） | POPE +4% | ❌ |
| **LCD** | 去掉图片（纯 LLM 输出） | CHAIR -36% | ❌ |
| **HALC** | 自适应焦点对比 + Beam Search | CHAIR 显著下降 | ❌ |

**优势**：即插即用，不需要重新训练模型，可以应用于任何 VLM。

#### 自我修正 (Self-Correction)

```
步骤 1: VLM 正常生成描述
步骤 2: 让 VLM 检查自己的描述 → "你确定图片里有狗吗？请再看一次。"
步骤 3: VLM 重新审视图片 → "抱歉，我看错了，图片里只有猫。"
```

### (c) 架构改进

#### 更强的视觉编码器

*   DINOv2 + SigLIP 双编码器提供更准确的视觉信息，减少 LLM "脑补"的空间。
*   更高分辨率（AnyRes）让模型看清细节，减少模糊导致的幻觉。

#### 更好的 Connector

*   MLP（无损传递）比 Q-Former（有损压缩）产生的幻觉更少。
*   信息越完整，LLM 越不需要"猜"。

## 6. 幻觉与 VLA 的关系

| VLM 幻觉类型 | 对 VLA 的影响 |
| :--- | :--- |
| **物体幻觉** | 机器人去抓不存在的物体 → 空抓或碰撞 |
| **位置幻觉** | 机器人去错误的位置操作 → 失败或损坏 |
| **关系幻觉** | 错误理解物体关系 → 放错位置 |
| **状态幻觉** | 以为任务完成了（实际没完成） → 提前终止 |

> 在 VLA 中，幻觉不只是"说错话"，而是**"做错事"**——代价从"尴尬"升级为"物理损坏"。

## 7. 发展趋势

```
2023:  发现 VLM 幻觉问题严重 (POPE 基准建立)
2024:  对比解码 (VCD, ICD, LCD) — 推理时方法，无需训练
2024:  V-DPO, LLaVA-RLHF — 训练时偏好学习
2025:  更强视觉编码器 (SigLIP 2) + 高分辨率 → 从源头减少幻觉
2025:  自验证机制 (GPT-5.2) → 模型内置事实核查
```

> **一句话总结**：多模态幻觉是 VLM 从"好玩"到"好用"的最大障碍——模型会"编造"图中不存在的内容。对抗幻觉需要从训练数据（去偏）、训练方法（RLHF/DPO）、推理策略（对比解码）、模型架构（更强编码器）四个维度同时发力。
