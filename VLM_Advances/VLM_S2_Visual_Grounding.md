# S2. 视觉接地：从"看图说话"到"指哪说哪"

## 1. 什么是 Visual Grounding（视觉接地）？

传统 VLM 只能输出**纯文本**——它可以说"图片里有一只猫"，但无法告诉你**猫在哪里**。

**Visual Grounding（视觉接地）** 让 VLM 具备了输出**空间坐标**的能力：

```
传统 VLM:
  用户: "图片里有什么动物？"
  模型: "一只橘色的猫。"        ← 只有文字

Grounded VLM:
  用户: "图片里有什么动物？"
  模型: "一只[橘色的猫](0.2, 0.3, 0.6, 0.8)。"  ← 文字 + 边界框坐标！
         └──── text ────┘└── bounding box ──┘
```

> **类比**：普通 VLM 像一个只会说"前面有障碍物"的副驾驶；Grounded VLM 像一个会说"前方 30 米偏左 15 度有一辆红色轿车"的副驾驶——精确定位才能真正有用。

## 2. 为什么 Grounding 对 VLM → VLA 至关重要？

| 应用场景 | 需要 Grounding？ | 原因 |
| :--- | :--- | :--- |
| **机器人操作** | ✅ 必须 | 机器人要抓杯子，必须知道杯子的精确位置 |
| **自动驾驶** | ✅ 必须 | 需要知道行人、车辆的精确位置和距离 |
| **UI 自动化** | ✅ 必须 | 点击按钮需要知道按钮的屏幕坐标 |
| **医学影像** | ✅ 必须 | 指出病灶的具体位置才有临床价值 |
| **聊天问答** | ❌ 不必须 | 纯文字回答就够了 |

**Grounding 是从 VLM（纯感知）到 VLA（感知+行动）的桥梁。**

## 3. 三种 Grounding 能力

### (a) Referring Expression Comprehension (REC)
**给定文字描述，输出对应物体的边界框。**

```
输入: "图中穿红衣服的女孩" + [图片]
输出: [0.15, 0.20, 0.45, 0.85]   ← (x1, y1, x2, y2) 边界框
```

### (b) Referring Expression Generation (REG)
**给定边界框，生成对应物体的文字描述。**

```
输入: [图片] + 边界框 [0.15, 0.20, 0.45, 0.85]
输出: "穿红色连衣裙、戴墨镜的年轻女孩"
```

### (c) Phrase Grounding
**在一段描述中，为每个名词短语标注对应的空间位置。**

```
输入: "一只[猫]坐在[桌子]上，旁边有一个[花瓶]。"
输出: "[猫](0.3, 0.4, 0.6, 0.7) [桌子](0.1, 0.5, 0.9, 0.95) [花瓶](0.7, 0.2, 0.85, 0.6)"
```

## 4. 代表模型

### (a) Kosmos-2（2023，Microsoft）

**核心创新**：将空间坐标表示为**特殊 Token**，嵌入在自然语言输出中。

**坐标编码方式**：
*   将图像空间离散化为 $32 \times 32$ 的网格。
*   每个位置对应一个特殊 Token：`<loc_0>` 到 `<loc_1023>`。
*   边界框用 4 个位置 Token 表示：`<loc_x1><loc_y1><loc_x2><loc_y2>`。

**输出格式**（类似 Markdown 超链接）：
```
"一只[可爱的猫](<loc_102><loc_156><loc_340><loc_498>)坐在桌上"
```

**训练数据**：构建了 **GrIT** 数据集——在网页图文对中自动标注 Grounding 信息。

### (b) Ferret（2023，Apple）

**核心创新**：支持**任意形状**的区域引用（不限于矩形框）。

```
输入方式：
  方式 1: 点击（Point）   → 用户点一个点
  方式 2: 矩形框（Box）   → 画一个框
  方式 3: 自由涂鸦（Scribble） → 随意圈画
  方式 4: 多边形（Polygon） → 精确描边
```

**Hybrid Region Representation**：将用户标注的区域采样为点集 + 区域特征，统一编码后输入 LLM。

### (c) CogAgent（2024，清华/智谱）

**核心创新**：专注于 **GUI/UI 理解和操作**。

```
输入: [手机截图] + "点击设置按钮"
输出: "设置按钮在屏幕 (0.85, 0.05) 位置" + 执行点击动作
```

**特点**：
*   高分辨率输入（1120x1120），能看清手机 UI 的小字和小按钮。
*   支持**网页操作、APP 操作**等 Agent 任务。
*   直接输出屏幕坐标，可以驱动自动化操作。

### (d) Set-of-Mark (SoM) Prompting（2023，Microsoft）

**核心创新**：不修改模型，而是在**输入图片上标注编号**。

```
原始图片 → 目标检测器 → 在每个物体上标注数字编号 → 标注后的图片
                                                        ↓
                         "请描述编号 3 的物体" → 普通 VLM → "编号 3 是一个红色杯子"
```

**优势**：无需训练新模型，任何 VLM 都能获得 Grounding 能力。

## 5. 坐标表示方法对比

| 方法 | 编码方式 | 精度 | 与 LLM 统一 | 代表 |
| :--- | :--- | :--- | :--- | :--- |
| **离散位置 Token** | 图像→32×32 网格→特殊 Token | 中 (~3%) | ✅ 完全统一 | Kosmos-2 |
| **归一化坐标文本** | 直接输出 "(0.35, 0.42)" 字符串 | 高 | ✅ 纯文本 | Qwen-VL, Ferret |
| **连续回归** | MLP 头直接回归坐标 | 最高 | ❌ 需额外头 | GLIP, GroundingDINO |
| **标记提示 (SoM)** | 输入图片上标编号 | N/A | ✅ 不改模型 | Set-of-Mark |

## 6. Grounding 对 VLA 的直接贡献

VLM 的 Grounding 能力直接为 VLA 提供了"在哪里操作"的信息：

```
VLM Grounding:    "红色杯子在 (0.35, 0.42, 0.55, 0.70)"
       ↓ 坐标转换（像素 → 3D 空间）
VLA Action:       "移动末端执行器到 (x=0.35m, y=-0.12m, z=0.15m)"
```

| VLM Grounding 能力 | 对应 VLA 任务 |
| :--- | :--- |
| 物体定位 (REC) | 知道"去哪里抓" |
| 空间关系理解 | 知道"杯子在盘子右边" |
| UI 元素定位 | 手机/电脑操作自动化 |
| 区域描述 (REG) | 机器人解释"我在看哪里" |

## 7. 发展趋势

| 趋势 | 说明 |
| :--- | :--- |
| **从 2D 到 3D** | 从平面边界框到 3D 空间坐标（深度估计 + Grounding） |
| **从静态到动态** | 视频中跟踪并定位物体（Video Grounding） |
| **从被动到主动** | 模型主动决定"看哪里"（Active Perception） |
| **与 VLA 深度融合** | Grounding 直接驱动机器人的运动规划 |

> **一句话总结**：Visual Grounding 让 VLM 从"知道是什么"进化到"知道在哪里"——这是 VLM 走向 VLA（动手操作）的必经之路。没有精确的空间定位，机器人就是一个"知道答案但找不到东西"的书呆子。
