# S5. 统一视觉模型：从"只看"到"又看又画"

## 1. 当前 VLM 的局限：只理解，不生成

截至 2024 年之前，主流 VLM（如 LLaVA, GPT-4V）的能力是**单向的**：

```
输入: [图片] + "描述这张图" → 🧠 VLM → 输出: "一只猫坐在红色沙发上"  ← 只能输出文字！
```

但人类的视觉能力是**双向的**——既能"看"（理解），也能"画"（生成）。能不能让 VLM 也做到这一点？

```
理解模式: [图片] + "这是什么？"     → "一只橘猫"
生成模式: "画一只橘猫坐在沙发上"    → [生成图片] 🎨
混合模式: [图片A] + "改成蓝色沙发"  → [生成图片B] 🎨
```

## 2. 为什么"统一"比"拼接"更好？

### 当前的拼接方案

目前大多数产品（如 ChatGPT）是将**理解模型**和**生成模型**拼接在一起：

```
理解: GPT-4V (VLM)          ← 独立模型
生成: DALL·E 3 (Diffusion)   ← 独立模型

流程: 用户说"画一只猫" → GPT-4V 写 Prompt → DALL·E 3 生成图片
```

**问题**：
1.  **信息损失**：VLM 理解图片后，只能用文字描述给生成模型，丢失了大量视觉细节。
2.  **不一致**：生成模型的"世界观"和理解模型不同，容易出错。
3.  **无法交互编辑**：想在图片的特定位置做精确修改很困难。

### 统一模型方案

```
输入: [图片/文字/混合]  → 🧠 统一模型 → 输出: [图片/文字/混合]
                              ↑
                    共享同一套表示空间
                    理解和生成用同一个模型
```

**优势**：
*   理解和生成共享世界知识。
*   可以做精确的**基于理解的编辑**。
*   单模型部署，更简单高效。

## 3. 核心技术：视觉 Tokenization

统一模型的关键技术是将图像**离散化为 Token**——与文本 Token 共享同一个词表和自回归框架。

### (a) VQ-VAE / VQGAN（视觉量化）

```
编码器:  [256×256 图片] → CNN Encoder → 连续特征 → 量化 → [16×16 离散 Token 网格]
                                                              ↑ 每个 Token 是码本中的一个编号
解码器:  [16×16 离散 Token 网格] → CNN Decoder → [重建的 256×256 图片]
```

**码本 (Codebook)**：包含 8192 个向量（相当于视觉的"词表"），每个 Token 就是最接近的码本条目的索引。

### (b) 统一词表

```
统一词表 = 文本 Token (32000 个) + 视觉 Token (8192 个)

输入序列:  [BOS] "画一只猫" [IMG_START] <v_3842> <v_1209> <v_5501> ... [IMG_END]
                   ↑ 文字          ↑ 生成的图像 Token（自回归）
```

## 4. 代表模型

### (a) Chameleon（2024，Meta）

**核心思想**：**从零开始联合训练**文本和图像 Token 的统一自回归模型。

```
[文本 Token] 和 [图像 Token] 混合在同一个序列中
→ 用标准 Transformer 做 Next Token Prediction
→ 模型自然地学会理解和生成
```

**关键创新**：
*   **Early Fusion**：图像 Token 和文本 Token 从第一层就开始交互（不是后融合）。
*   **联合训练**：在 4.4 万亿 Token（文本 + 图像）上从零训练。
*   训练稳定性技巧：QK-Norm、Dropout 等防止多模态训练崩溃。

**模型规模**：7B 和 34B。

### (b) Emu / Emu2（2023-2024，BAAI）

**核心思想**：在预训练好的 LLM 基础上**加入视觉生成能力**。

```
Stage 1: 用 LLM (LLaMA) 初始化，加入视觉编码器，学会理解
Stage 2: 加入视觉解码器 (Diffusion)，学会生成
Stage 3: 指令微调，学会根据指令在理解和生成之间切换
```

**Emu2 的能力**：
*   看图回答问题（理解）
*   根据文字生成图片（生成）
*   根据图片生成新图片（视觉编辑）
*   交错的图文生成（图文混排）

### (c) Show-o（2024）

**核心思想**：在一个 Transformer 中同时使用**自回归**（文本）和**离散扩散**（图像）。

```
文本 Token: 自回归生成（从左到右）
图像 Token: 离散扩散生成（并行去噪）

两种生成方式在同一个模型中共存
→ 文本部分享受自回归的流畅性
→ 图像部分享受扩散的高质量
```

### (d) Janus（2024-2025，DeepSeek）

**核心思想**：为理解和生成使用**不同的视觉编码方式**，但共享 LLM 骨干。

```
理解路径: [图片] → SigLIP (语义编码) → LLM → 文字输出
生成路径: "画一只猫" → LLM → VQ Token → VQGAN Decoder → 图片输出
                        ↑
                 共享 LLM 骨干
```

**为什么分开编码？** 理解需要高层语义（SigLIP），生成需要底层像素细节（VQ-VAE）——两者需求不同，用同一个编码器会相互妥协。

## 5. 各方案对比

| 模型 | 统一方式 | 理解能力 | 生成能力 | 架构复杂度 |
| :--- | :--- | :--- | :--- | :--- |
| **Chameleon** | 完全统一（Joint Training） | 中 | 中 | 简单 |
| **Emu2** | 分阶段加入（Sequential） | 强 | 强 | 中等 |
| **Show-o** | 自回归 + 离散扩散混合 | 强 | 强 | 中等 |
| **Janus** | 理解/生成分离编码 | **最强** | 强 | 低 |

## 6. 与 VLM/VLA 的关系

### 对 VLM 的价值

统一模型扩展了 VLM 的能力边界——不仅能"描述"图片，还能"修改"和"创造"：

```
传统 VLM:   "图里有一只猫" → 纯文字
统一模型:   "图里有一只猫" → 文字
            "把猫的颜色改成黑色" → 生成新图片
            "画一只类似的猫在花园里" → 生成新图片
```

### 对 VLA 的价值

*   **视觉目标生成**：VLA 可以先生成"任务完成后的目标图片"，再规划如何达到该状态。
*   **数据增广**：生成更多训练场景（不同光照、物品、背景），扩充机器人训练数据。
*   **世界模型**：统一模型可以预测"执行动作后世界会变成什么样"（视频预测）。

## 7. 发展趋势

```
2023:  Emu                ← LLM + 视觉生成，早期探索
2024:  Chameleon          ← 完全统一训练的先驱
2024:  Show-o, Janus      ← 更精巧的混合架构
2025:  GPT-4o 原生图片生成 ← 商业化落地
2025:  Janus-Pro          ← 理解生成分离编码，两不耽误
趋势:  统一多模态 = 文字 + 图片 + 视频 + 音频 + 动作 (VLA)
```

> **一句话总结**：统一视觉模型让 AI 从"只看不画"进化到"又看又画"——通过视觉 Tokenization 将图像纳入自回归框架，实现理解与生成的统一。这不仅扩展了 VLM 的能力边界，也为 VLA 的视觉目标生成和世界模型提供了技术基础。
