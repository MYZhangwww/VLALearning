# S3. 视频理解：从单张图片到时间流

## 1. 为什么视频理解是 VLM 的关键拼图？

现实世界是**动态的**——物体在移动、事件在发生、状态在变化。单张图片只是时间的一个切片。

```
单图 VLM:   [📷 一个人举着手] → "一个人在举手"     ← 但他是在打招呼？还是在求救？
视频 VLM:   [📷📷📷📷 连续帧]  → "一个人在向朋友挥手告别"  ← 有了时间信息才能判断
```

**对 VLA 尤其重要**：机器人需要理解**动态场景**——物体在移动、人在走动、任务进展在变化。

## 2. 视频理解的核心挑战

| 挑战 | 说明 |
| :--- | :--- |
| **Token 爆炸** | 1 张图 = ~576 Token；1 分钟视频 (30fps) = 1800 帧 × 576 = **100 万+ Token** |
| **时间建模** | 需要理解帧与帧之间的因果关系和时间顺序 |
| **关键帧选择** | 大部分帧是冗余的，如何选出最有信息量的帧？ |
| **长视频** | 电影（2 小时）、会议（数小时）需要处理超长序列 |
| **时间定位** | 不仅要理解"发生了什么"，还要知道"在第几秒发生的" |

## 3. 帧采样策略

### (a) 均匀采样 (Uniform Sampling)

最简单的方法：每隔 N 帧取一帧。

```
原始: [F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 ...]  (30 fps)
采样: [F1         F4         F7         F10      ...]  (每 3 帧取 1 帧 → 10 fps)
```

*   **优点**：简单，覆盖面均匀。
*   **缺点**：可能错过关键时刻（如"球进门"的瞬间）。

### (b) 关键帧选择 (Keyframe Selection)

根据视觉变化程度选择信息量最大的帧：

```
原始: [走路 走路 走路 跳起 落地 走路 走路 踢球 进球 庆祝]
关键帧:                   [跳起]        [踢球] [进球] [庆祝]
```

*   **方法**：基于帧间差异（光流、CLIP 特征相似度）选择变化最大的帧。
*   **优点**：保留最关键的信息。
*   **缺点**：计算成本高，可能遗漏缓慢变化。

### (c) 自适应帧选择 (Adaptive Selection)

**代表**：ReWind（2024）

动态学习型记忆模块，通过"读-感知-写"循环自适应选帧：
*   比均匀采样在 MovieChat VQA 上提升 **+13%**。
*   在时间定位任务上 mIoU 提升 **+8%**。

## 4. 视频 Token 的压缩策略

即使采样了关键帧，Token 数量仍然巨大。需要进一步压缩：

| 策略 | 说明 | 代表 |
| :--- | :--- | :--- |
| **时间池化 (Temporal Pooling)** | 相邻帧的 Token 做平均/最大池化 | 多数模型 |
| **Token 合并 (Token Merging)** | 合并相似的 Token，减少冗余 | ToMe |
| **Token 乱序压缩 (Shuffling)** | 打乱 Token 顺序后压缩，保留关键信息 | TimeSuite |
| **Q-Former 压缩** | 用 Query Transformer 将每帧压缩为固定 Token | Video-LLaVA |
| **记忆模块 (Memory)** | 用循环结构维护压缩的长期记忆 | ReWind |

## 5. 时间位置编码

LLM 需要知道"这一帧是视频的第几秒"——这需要专门的时间位置编码。

| 方法 | 说明 | 代表 |
| :--- | :--- | :--- |
| **Frame Index Token** | 在每帧前插入帧编号 Token：`<frame_3>` | Video-ChatGPT |
| **TAPE** | Temporal Adaptive Position Encoding，自适应时间位置编码 | TimeSuite |
| **M-RoPE (3D)** | 将 RoPE 扩展为 (Time, Height, Width) 三维 | **Qwen2-VL** |
| **文本时间对齐** | 用文本描述时间戳，对齐视频帧 | Qwen3-VL |
| **BiLSTM** | 用 LSTM 聚合时间特征 | TemporalVLM |

**M-RoPE** 是目前最优雅的方案——它将位置编码统一到了 (T, H, W) 三维空间，图片就是 T=1 的特例，视频是 T>1 的自然扩展。

## 6. 代表模型

### Video-LLaVA (2023)
*   **方法**：均匀抽 8 帧，每帧通过 ViT 编码，拼接后输入 LLM。
*   **限制**：8 帧对短视频够用，但无法处理长视频。

### VideoChat2 (2024)
*   **方法**：Q-Former 每帧压缩 + 时间池化。
*   **能力**：支持视频对话和问答。

### Qwen2-VL (2024)
*   **方法**：M-RoPE 统一图像和视频的位置编码，动态分辨率。
*   **突破**：视频帧当作"时间维度更长的图片"来处理，架构统一优雅。

### Gemini 1.5 Pro (2024)
*   **方法**：1M Token 上下文 + 原生视频编码 + 音频波形特征。
*   **突破**：可以处理**数小时**的视频，不需要采样，全部帧编码输入。

### Gemini 3 Pro (2025)
*   **方法**：帧级推理 + 音视频同步分析。
*   **突破**：Video-MMMU 基准 87.6%，视频理解最强。

## 7. 视频理解的任务类型

| 任务 | 描述 | 难度 |
| :--- | :--- | :--- |
| **视频描述 (Video Captioning)** | 生成视频的文字描述 | ⭐⭐ |
| **视频问答 (Video QA)** | 回答关于视频的问题 | ⭐⭐⭐ |
| **时间定位 (Temporal Grounding)** | "这件事在第几秒发生？" | ⭐⭐⭐⭐ |
| **动作识别 (Action Recognition)** | 识别视频中的动作类别 | ⭐⭐⭐ |
| **长视频理解** | 理解数小时的视频内容 | ⭐⭐⭐⭐⭐ |
| **视频推理** | 基于视频内容进行因果推理 | ⭐⭐⭐⭐⭐ |

## 8. 视频理解对 VLA 的价值

```
VLM 视频理解:    理解"人在演示如何折叠毛巾" → 学习操作步骤
                                               ↓
VLA 模仿学习:    模仿视频中的动作 → 机器人也学会折叠毛巾
```

| VLM 视频能力 | VLA 应用 |
| :--- | :--- |
| 动作识别 | 从演示视频中学习操作技能 |
| 时间推理 | 理解任务的时序（先拿杯子，再倒水） |
| 状态变化检测 | 判断任务是否完成（桌子干净了吗？） |
| 长程理解 | 复杂多步骤任务的全流程理解 |

## 9. 发展趋势

| 趋势 | 说明 |
| :--- | :--- |
| **超长视频 → 标配** | 从 8 帧 → 数小时，Gemini 已实现 |
| **音视频联合** | 同时理解画面和声音（Qwen3-Omni） |
| **时间精度提升** | 从"大概在中间"到"第 3 分 22 秒" |
| **实时视频流** | 从离线分析到在线实时理解 |
| **视频 → VLA** | 从观看视频学习操作（Video Imitation Learning） |

> **一句话总结**：视频理解让 VLM 从"看照片"进化到"看电影"——通过帧采样、Token 压缩、时间位置编码等技术，VLM 获得了理解动态世界的能力，这是连接静态感知和动态行动（VLA）的关键桥梁。
