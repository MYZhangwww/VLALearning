# 27. VLA SOTA 模型全景对比（截至 2026 年 2 月）

## 1. VLA 模型全家福

截至 2026 年 2 月，VLA 领域已经涌现出多个有影响力的模型。下面是按时间线梳理的主要模型：

```
2022.12  RT-1 (Google)         ← 首个大规模 Robotics Transformer
2023.07  RT-2 (Google)         ← 首个 VLA，动作即语言
2023.10  RT-2-X (Google)       ← RT-2 的跨具身版本
2024.01  Octo (Berkeley)       ← 开源通用策略，扩散动作头
2024.06  OpenVLA (Stanford)    ← 开源 7B VLA，超越 RT-2-X
2024.10  π0 (Physical Intel.)  ← 通用机器人基座，扩散策略
2024.12  GR-2 (ByteDance)      ← 视频生成式世界模型 + 机器人控制
2025.01  π0-FAST (Physical I.) ← FAST 编码加速，5x 训练提速
2025.02  OpenVLA-OFT (Stanford)← 优化版微调，97.1% LIBERO
2025.04  π0.5 (Physical Intel.)← 首次开放世界泛化
2025.H2  EdgeVLA / LiteVLA     ← 边缘部署 VLA
2025.09  Discrete Diffusion VLA← 统一框架的离散扩散
```

## 2. 核心模型横向对比

| 特性 | RT-2-X | Octo | OpenVLA | OpenVLA-OFT | π0 | π0.5 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **发布** | 2023.10 | 2024.01 | 2024.06 | 2025.02 | 2024.10 | 2025.04 |
| **机构** | Google | Berkeley | Stanford | Stanford | Physical Intel. | Physical Intel. |
| **参数量** | 55B | 93M | 7B | 7B | ~3-7B (未公开) | ~3-7B (未公开) |
| **视觉编码** | ViT-G | ViT (from scratch) | DINOv2+SigLIP | DINOv2+SigLIP | ViT | ViT |
| **LLM 骨干** | PaLI-X | 无 (小 Transformer) | Llama 2 7B | Llama 2 7B | Pre-trained VLM | Pre-trained VLM |
| **动作头** | 自回归 Bin | **扩散** | 自回归 Bin | **MLP 回归** | **扩散** | **扩散** |
| **Action Chunk** | 1 | 4 | 1 | 多步 | 16-50 | 16-50 |
| **开源** | ❌ | ✅ | ✅ | ✅ | ✅ (部分) | 部分 |

## 3. 性能基准对比

### (a) LIBERO 仿真基准

LIBERO 是 VLA 评测的标准仿真基准，包含多组不同难度的任务：

| 模型 | LIBERO-Spatial | LIBERO-Object | LIBERO-Goal | LIBERO-Long | **平均** |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Diffusion Policy | 78.3% | 92.5% | 68.3% | 50.5% | 72.4% |
| Octo-Base | 78.9% | 85.7% | 84.6% | 51.1% | 75.1% |
| OpenVLA | 84.7% | 88.5% | 79.2% | 53.7% | 76.5% |
| π0 | — | — | — | — | ~85% (估计) |
| **OpenVLA-OFT** | **96.7%** | **98.3%** | **97.5%** | **96.0%** | **97.1%** ✅ |

**OpenVLA-OFT 的成功率接近完美**，说明优化后的微调方案可以极大释放 VLA 的潜力。

### (b) SimplerEnv 仿真基准

| 模型 | Fractal | Bridge | 说明 |
| :--- | :--- | :--- | :--- |
| RT-2-X | 基线 | 基线 | 闭源 55B |
| Octo | 较低 | 较低 | 93M 小模型 |
| OpenVLA | +16.5% | 领先 | 7B 开源 |
| Discrete Diffusion VLA | **71.2%** | **49.3%** | 统一框架 |

### (c) 真实机器人成功率

| 模型 | 测试任务数 | 平均成功率 | 机器人平台 |
| :--- | :--- | :--- | :--- |
| RT-2-X | 数十个 | 基线 | Google Robot |
| OpenVLA | 29 | RT-2-X **+16.5%** | WidowX + Franka |
| π0 | 数百种 | 高（具体未公开） | 多种平台 |
| π0.5 | 长程家务 | **首次开放世界成功** | 多种平台 |
| OpenVLA-OFT | 双臂任务 | 超越 π0 (ALOHA 基准) | ALOHA 双臂 |

## 4. 各模型深度分析

### RT-2-X — 先驱者（已过时）
*   **历史意义**：首个证明 VLM 知识可迁移到机器人的模型。
*   **当前地位**：已被 OpenVLA 等后续工作全面超越。
*   **主要问题**：55B 参数太大，闭源，推理极慢。

### Octo — 轻量级通用策略
*   **定位**：93M 参数的轻量级通用策略，面向学术研究。
*   **优势**：极小的模型，支持多机器人，开源友好。
*   **劣势**：没有 LLM 骨干，语言理解和推理能力弱。

### OpenVLA — 开源标杆
*   **定位**：VLA 领域的 "LLaMA"——开源、高效、人人可用。
*   **优势**：DINOv2+SigLIP 双编码器、LoRA 微调友好、消费级 GPU 可部署。
*   **劣势**：自回归 Binning 精度有限、逐步生成速度慢。

### OpenVLA-OFT — 优化后的王者
*   **定位**：OpenVLA 的增强版，通过四项优化达到接近完美的性能。
*   **优势**：LIBERO 97.1%、26x 吞吐量提升、超越 π0。
*   **劣势**：仅在仿真基准中验证，真实世界大规模部署待确认。

### π0 / π0.5 — 产业级通用基座
*   **定位**：最接近"通用机器人大脑"的模型。
*   **优势**：10000+ 小时数据、扩散策略精度高、双臂支持、开放世界泛化 (π0.5)。
*   **劣势**：未完全开源、训练数据不可复现、扩散推理慢。

### GR-2 — 视频生成路线
*   **定位**：ByteDance 的另一条路线——通过视频生成来做机器人控制。
*   **核心思想**：先生成"理想视频"（目标状态），再用逆动力学模型将视频转化为动作。
*   **优势**：可以利用互联网视频数据预训练。
*   **劣势**：视频生成质量和物理一致性仍是挑战。

## 5. 关键维度对比表

### (a) 架构维度

| 模型 | 视觉编码器 | LLM 骨干 | 动作表示 | Action Chunk |
| :--- | :--- | :--- | :--- | :--- |
| RT-2-X | ViT-G | PaLI-X 55B | 自回归 256 Bin | 1 |
| Octo | ViT (自训练) | 小 Transformer | 扩散 | 4 |
| OpenVLA | DINOv2+SigLIP | Llama 2 7B | 自回归 256 Bin | 1 |
| OFT | DINOv2+SigLIP | Llama 2 7B | MLP 回归 | 多步 |
| π0 | ViT | VLM (未公开) | 扩散 | 16-50 |
| π0-FAST | ViT | VLM (未公开) | FAST Token | 16-50 |

### (b) 数据与泛化维度

| 模型 | 训练数据量 | 跨机器人 | 跨环境 | 新指令泛化 | 开放世界 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| RT-2-X | 中等 + 互联网 VL | ✅ | 有限 | ✅ | ❌ |
| Octo | OXE 子集 | ✅ | 有限 | 有限 | ❌ |
| OpenVLA | 970K 轨迹 | ✅ | 有限 | ✅ | ❌ |
| π0 | 10,000+ 小时 | ✅ | 中等 | ✅ | ❌ |
| **π0.5** | 更大 | ✅ | **✅ 全新家庭** | ✅ | **✅** |

### (c) 部署维度

| 模型 | 参数量 | 推理硬件 | 实时性 | 开源 |
| :--- | :--- | :--- | :--- | :--- |
| RT-2-X | 55B | TPU 集群 | 慢 | ❌ |
| Octo | 93M | 单 GPU | 快 | ✅ |
| OpenVLA | 7B | A100 / 4090 | 中等 | ✅ |
| OFT | 7B | A100 / 4090 | **26x 更快** | ✅ |
| π0 | ~3-7B | GPU | 中等 (扩散) | ✅ 部分 |
| EdgeVLA | ~1-3B | 消费级 GPU | **实时** | ✅ |
| LiteVLA | <1B | **CPU** | 实时 | ✅ |

## 6. 选型指南

| 需求 | 推荐模型 | 理由 |
| :--- | :--- | :--- |
| **学术研究/复现** | **OpenVLA** | 完全开源，文档完善，消费级 GPU 可跑 |
| **追求最高精度** | **OpenVLA-OFT** | LIBERO 97.1%，优化后性能最强 |
| **产业级多任务部署** | **π0 / π0.5** | 最大规模数据、最强泛化、多机器人支持 |
| **边缘/低成本部署** | **EdgeVLA / LiteVLA** | 消费级硬件甚至 CPU 即可运行 |
| **轻量级实验** | **Octo** | 93M 参数，训练和部署成本最低 |
| **视频生成路线** | **GR-2** | 可利用互联网视频，新颖范式 |
| **灵巧操作** | **π0 (扩散版)** | 扩散策略精度最高，适合精细任务 |

## 7. VLA 领域的未来方向

| 方向 | 说明 | 代表工作 |
| :--- | :--- | :--- |
| **开放世界泛化** | 从实验室到真实家庭，从已知物体到未知物体 | π0.5 |
| **世界模型 + VLA** | 先预演再执行，实现长程规划 | World-VLA-Loop |
| **人形机器人** | 从桌面机械臂扩展到双足人形机器人（全身控制） | Figure, Optimus, 1X |
| **数据飞轮** | 部署采集 → 训练 → 再部署的自动循环 | Tesla Optimus |
| **多模态VLA** | 加入触觉、力觉等更多感官模态 | 前沿研究 |
| **安全可靠** | 可预测、可解释、有安全保障的机器人行为 | 受限策略 + 安全层 |

## 8. 总结

| 维度 | 当前最强 | 说明 |
| :--- | :--- | :--- |
| **仿真基准** | **OpenVLA-OFT** (97.1%) | 优化后的微调方案释放了巨大潜力 |
| **真实世界泛化** | **π0.5** | 唯一实现开放世界泛化的模型 |
| **开源生态** | **OpenVLA** | 最完善的开源代码、权重、文档 |
| **部署效率** | **EdgeVLA / LiteVLA** | CPU 级别的实时控制 |
| **操作精度** | **π0 (扩散版)** | 扩散策略天然高精度 |
| **数据规模** | **π0 系列** (10000+ 小时) | 目前最大规模的训练数据 |

> **一句话总结**：VLA 领域正处于从"实验室 demo"到"真实世界部署"的关键转折点——OpenVLA 降低了研究门槛，π0.5 突破了泛化天花板，EdgeVLA 解决了部署瓶颈。下一步是让 VLA 机器人走进千家万户，这需要数据飞轮、世界模型、安全机制的共同进步。
