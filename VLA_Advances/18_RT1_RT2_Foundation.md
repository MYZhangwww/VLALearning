# 18. RT-1 到 RT-2：VLA 的诞生

## 1. 背景：为什么 Google DeepMind 要做 Robotics Transformer？

在 2022 年之前，机器人学习的主流方法是**小模型 + 小数据集 + 单任务**：
*   每个任务（如"抓取红色方块"）训练一个独立的小策略网络。
*   换一个物体（蓝色方块）就要重新训练。
*   完全没有语言理解能力——你不能用自然语言下指令。

Google DeepMind 的核心洞察：**NLP 领域的 Scaling Law（大数据 + 大模型 = 涌现能力）也许在机器人领域同样成立。**

## 2. RT-1：Robotics Transformer（2022.12）

### (a) 核心思想
用 **Transformer** 替代传统的小型策略网络，在**大规模机器人数据**上训练。

### (b) 架构

```
输入：
  [RGB 图像 (300x300)] + [语言指令 "pick up the can"]
      ↓
  图像编码器 (EfficientNet-B3) → 视觉 Token
      ↓
  语言编码器 (FiLM) → 条件注入
      ↓
  Transformer Decoder (8 层) → TokenLearner 压缩
      ↓
输出：
  离散化的动作 Token → 解码为 [Δx, Δy, Δz, Δyaw, gripper, mode]
```

**关键设计**：
*   **动作离散化**：将连续动作空间每个维度切分为 256 个 Bin，转化为分类问题。
*   **TokenLearner**：将视觉 Token 从 81 个压缩到 8 个，大幅降低计算量。
*   **FiLM 条件注入**：语言指令通过 Feature-wise Linear Modulation 注入视觉特征。

### (c) 数据规模

| 指标 | 数值 |
| :--- | :--- |
| 训练数据 | **130,000 条演示轨迹** |
| 任务数量 | **700+ 不同任务** |
| 采集机器人 | 13 台 Everyday Robot |
| 采集时间 | 17 个月 |
| 环境 | Google 办公室厨房 |

### (d) 关键成果
*   在训练任务上成功率 **97%**。
*   对新物体的零样本泛化率 **76%**（之前的方法 < 30%）。
*   首次证明了**大规模机器人数据 + Transformer 架构的威力**。

### (e) 局限
*   **不理解语义**：它只是学会了"指令字符串 → 动作"的映射，并没有真正理解"can"是什么。
*   **泛化有限**：面对训练中从未出现过的指令（如"pick up something heavy"），完全无法处理。
*   **知识孤岛**：模型只在机器人数据上训练，没有利用互联网上的海量视觉-语言知识。

---

## 3. RT-2：Vision-Language-Action Model（2023.07）

### (a) 核心突破

RT-2 的核心洞察震撼了整个机器人学界：

> **"不需要从头训练机器人模型——直接拿一个已有的 VLM，把机器人动作当成'另一种语言'来输出就行了。"**

具体做法：
1.  取一个在互联网数据上预训练好的 VLM（PaLI-X 55B 或 PaLM-E 12B）。
2.  把机器人动作**编码为文本字符串**：`[0.02, -0.01, 0.05, 0, 0.1, 0, 1]` → `"2 -1 5 0 10 0 1"`。
3.  在机器人数据上 **co-fine-tune**，让模型同时保持 VQA 能力和学习机器人控制。

### (b) 架构

```
输入：
  [RGB 图像] + "pick up the bottle that is closest to the edge of the table"
      ↓
  预训练 VLM (PaLI-X 55B / PaLM-E 12B)
      ↓
  自回归输出 Token 序列
      ↓
输出（两种模式）：
  模式 1 (VQA): "The bottle is a blue plastic water bottle..."
  模式 2 (Action): "1 128 91 241 5 101 128"  ← 这就是动作！
```

**关键创新：动作即语言 (Action as Language)**
*   每个动作维度被量化为 [0, 255] 的整数。
*   7 维动作变成 7 个整数 Token，直接接在语言 Token 后面自回归生成。
*   模型无需任何架构修改——动作只是"另一种语言"。

### (c) 对比 RT-1 vs RT-2

| 特性 | RT-1 | RT-2 |
| :--- | :--- | :--- |
| **底层模型** | 从零训练的 Transformer | **预训练 VLM (PaLI-X / PaLM-E)** |
| **参数量** | ~35M | **12B - 55B** |
| **训练数据** | 仅机器人数据 (130K) | **互联网 VL 数据 + 机器人数据** |
| **语义理解** | ❌ 字符串匹配 | ✅ 真正理解语义 |
| **新指令泛化** | ❌ | ✅ 零样本理解从未见过的指令 |
| **动作输出** | 分类头 (每维 256 Bin) | **文本 Token 自回归** |
| **涌现能力** | ❌ | ✅ 推理、类比、链式思考 |

### (d) RT-2 的涌现能力（最惊艳的部分）

RT-2 展现了 RT-1 完全不具备的**涌现能力 (Emergent Capabilities)**：

| 涌现能力 | 举例 | 为什么 RT-1 做不到 |
| :--- | :--- | :--- |
| **语义推理** | "把最小的物体放到碗里" — 模型能比较大小 | RT-1 不知道"最小"是什么意思 |
| **符号理解** | "把一个国家的国旗移到另一个" — 模型认识国旗 | RT-1 没有国旗的知识 |
| **类比推理** | "把可以当锤子的东西递给我" — 模型挑了石头 | RT-1 不理解"功能类比" |
| **Chain-of-Thought** | 内部推理链："这个场景里...石头最硬最重...适合当锤子" | RT-1 没有推理能力 |

这些能力**不是在机器人数据中学到的**——它们来自 VLM 在互联网上学到的世界知识，通过 co-fine-tuning 被"激活"并连接到了机器人的动作空间。

### (e) RT-2-X：跨具身扩展

RT-2-X 是 RT-2 在 **Open X-Embodiment** 数据集（多个机器人平台）上训练的版本：
*   在原始 RT-2 的数据基础上，加入了来自其他实验室的多机器人数据。
*   在不同机器人上的泛化性能提升了 **3 倍**。
*   成为后续 OpenVLA 等工作的对标基线。

---

## 4. 为什么 RT-2 是里程碑？

RT-2 的意义不在于某个具体指标提升了多少，而在于它**证明了一个范式**：

```
互联网知识（VLM）→ 物理动作（VLA）
```

具体来说：
1.  **VLM 的知识是可迁移的**：在互联网上学到的"杯子长什么样"直接帮助机器人抓杯子。
2.  **动作可以被编码为 Token**：不需要为机器人设计专门的架构，通用的自回归框架就够了。
3.  **Scaling Law 适用于机器人**：更大的 VLM 底座 → 更强的机器人控制能力。

> **一句话总结**：RT-1 证明了"大数据 + Transformer"对机器人有效；RT-2 则证明了"VLM 的知识可以直接驱动机器人"——这一洞察开创了整个 VLA 领域。
