# 24. 世界模型与机器人规划

## 1. 什么是世界模型？

**世界模型 (World Model)** 是一种能够**预测环境未来状态**的模型。对于机器人来说，就是"先在脑中预演"：

```
当前观测                                预测的未来
[📷 桌上有杯子]  → 🧠 世界模型 →  [📷 杯子被推到桌边]  → [📷 杯子掉落!]
                   "如果我向右推..."     ↑ 预测帧 1           ↑ 预测帧 2
```

> **类比**：你在倒水之前，会在脑中想象水流出来的样子。如果脑中预演发现会洒出来，你就调整手的角度。世界模型就是让机器人拥有这种"预演"能力。

## 2. 为什么 VLA 需要世界模型？

### 反应式 VLA 的局限

当前主流 VLA（如 OpenVLA, π0）是**反应式 (Reactive)** 的：

```
反应式 VLA:   观测 → 模型 → 动作 → 执行 → 观测 → 模型 → 动作 → ...
                                                    ↑ 没有"想"的过程
```

问题：
1.  **无法预见后果**：不知道当前动作会导致什么结果。
2.  **短视 (Myopic)**：只关注下一步，不考虑长期目标。
3.  **无法规划**：面对需要多步协调的任务（如"先清空桌子，再擦桌子，再摆放餐具"），效率很低。

### 规划式 VLA（有世界模型）

```
规划式 VLA:
  观测 → 🧠 世界模型 → 预测未来 N 步 → 评估哪条路径最好 → 执行最优动作
                         ↑ "想"了很多步
```

## 3. 视频预测作为世界模型

### (a) 核心思想

最直观的世界模型形式：**预测未来的视频帧**。

```
当前帧 + 动作 → Video World Model → 未来帧序列
[t=0]   [a0]                        [t=1] [t=2] [t=3] ...
```

如果模型能准确预测"执行动作 a 后，世界变成什么样"，那它就理解了物理世界的因果关系。

### (b) 代表方法

#### UniPi (Universal Policy via Video Prediction)

*   **思路**：先用视频扩散模型生成"目标视频"（从当前帧到任务完成的理想视频），再用逆动力学模型将视频转换为动作。
*   **流水线**：`[当前帧 + 指令] → 视频扩散模型 → [预测视频] → 逆动力学模型 → [动作序列]`
*   **优势**：视频数据在互联网上很丰富（YouTube），可以利用大规模视频预训练。

#### iVideoGPT (Interactive Video GPT)

*   **思路**：将视觉观测、动作、奖励统一编码为 Token 序列，用自回归 Transformer 建模。
*   **数据**：在**数百万条人类和机器人操作轨迹**上预训练。
*   **能力**：支持条件视频预测、视觉规划、基于模型的强化学习。

#### ViPRA (Video Prediction for Robot Actions)

*   **思路**：从**无动作标注的视频**中学习机器人控制。
*   **创新**：通过光流一致性提取"隐式动作"——不需要知道真实动作标签。
*   **成果**：SIMPLER 基准提升 16%，真实机器人操作提升 13%。

## 4. World-VLA-Loop：闭环学习（2026 年最新）

### (a) 核心思想

**World-VLA-Loop** 将世界模型和 VLA 策略放在一个**闭环**中迭代优化：

```
┌─────────────────────────────────────────────────────┐
│                  World-VLA-Loop                      │
│                                                     │
│  VLA 策略 ──→ 在虚拟环境中执行 ──→ 收集经验        │
│      ↑                                    ↓         │
│      │            ┌──────────────┐                  │
│      │            │ 视频世界模型  │ ← 从经验中学习   │
│      │            │ (预测未来)    │                  │
│      │            └──────┬───────┘                  │
│      │                   ↓                          │
│      └──── RL 优化 ←── 奖励信号                     │
│                                                     │
└─────────────────────────────────────────────────────┘
```

**关键流程**：
1.  **VLA 策略**在世界模型生成的虚拟环境中探索。
2.  **世界模型**预测未来帧和奖励。
3.  用预测的奖励通过 **RL** 优化 VLA 策略。
4.  VLA 策略的失败案例反馈给世界模型，**迭代提升世界模型的精度**。

### (b) 为什么是闭环的？

*   **世界模型学习 VLA 的弱点**：VLA 经常失败的场景，世界模型会重点学习预测。
*   **VLA 在更准确的世界中练习**：世界模型越准，VLA 的 RL 训练越有效。
*   两者**互相提升**，形成正反馈循环。

## 5. 世界模型的关键挑战

| 挑战 | 说明 | 现状 |
| :--- | :--- | :--- |
| **长时预测漂移** | 预测越远越不准确，误差累积 | 通过重规划 (Replanning) 缓解 |
| **物理真实性** | 生成的视频可能违反物理定律（物体穿模、悬浮） | 引入物理先验约束 |
| **计算成本** | 视频生成模型推理非常慢 | 压缩模型、潜空间预测 |
| **泛化** | 在新环境中预测不准 | 大规模视频预训练 |
| **接触动力学** | 碰撞、摩擦等接触物理极难预测 | 仍是开放问题 |

## 6. 各方法对比

| 方法 | 输入 | 输出 | 是否需要动作标注 | 特点 |
| :--- | :--- | :--- | :--- | :--- |
| **UniPi** | 当前帧 + 指令 | 目标视频 → 动作 | 逆动力学需要 | 可利用互联网视频 |
| **iVideoGPT** | 观测 + 动作 | 未来帧 + 奖励 | ✅ 需要 | 统一 Token 建模 |
| **ViPRA** | 无标注视频 | 隐式动作 + 预测帧 | ❌ 不需要 | 数据需求最低 |
| **World-VLA-Loop** | VLA 经验 | 虚拟环境 + 奖励 | VLA 自动生成 | 闭环迭代优化 |

## 7. 世界模型 vs 直接策略学习

| 特性 | 直接策略学习 (OpenVLA, π0) | 世界模型 + 规划 |
| :--- | :--- | :--- |
| **数据效率** | 需要大量演示 | 可以在虚拟中"想象"练习，数据更省 |
| **长程推理** | 弱（短视） | **强（可以预演多步）** |
| **推理速度** | 快 | 慢（需要生成视频） |
| **成熟度** | 高（已有成功部署） | 低（仍在研究阶段） |
| **安全性** | 需要真实试错 | 可以在脑中试错，更安全 |

## 8. 发展趋势

```
2023:  UniPi, SuSIE     ← 视频预测 → 动作的先驱
2024:  iVideoGPT        ← 统一 Token 建模
2025:  ViPRA            ← 从无标注视频学习
2026:  World-VLA-Loop   ← 🔑 世界模型 + VLA 闭环
```

> **一句话总结**：世界模型让 VLA 从"反应式"进化为"规划式"——先在脑中预演，再在现实中执行。这是 VLA 从"做简单任务"到"做复杂长程任务"的关键一步，也是当前研究的最前沿。
