# 19. VLA 架构深度解析：如何把 VLM 变成机器人控制器

## 1. VLA 的通用架构模板

几乎所有 VLA 模型都遵循相同的三段式流水线：

```
┌──────────────┐   ┌──────────────────────┐   ┌───────────────────┐
│ 视觉编码器    │   │ 骨干网络 (Backbone)    │   │ 动作解码头         │
│ Vision Encoder│──→│ (通常是 LLM)          │──→│ Action Head        │
│ ViT / CLIP   │   │ Llama / PaLM / Qwen  │   │ AR / Diffusion     │
└──────────────┘   └──────────────────────┘   └───────────────────┘
        ↑                    ↑                          ↓
   [RGB 图像]         [语言指令]                [机器人动作序列]
                    [本体感受(可选)]
```

每段的职责：
*   **视觉编码器**：把 RGB 图像变成视觉 Token 序列
*   **骨干 LLM**：融合视觉 + 语言 + 本体感受，进行多模态推理
*   **动作解码头**：把 LLM 的隐藏状态转换为机器人可执行的动作

## 2. 核心问题：自回归模型为什么无法获取联合分布？

在深入五大架构之前，必须先理解一个关键问题：**自回归模型在建模机器人动作时的根本缺陷——无法正确捕获动作维度间的联合分布。**

### (a) 什么是联合分布？

机器人的一个动作通常是一个多维向量，例如 7 维：

```
动作 a = [Δx, Δy, Δz, Δroll, Δpitch, Δyaw, gripper]
         位置变化(3维)   旋转变化(3维)    夹爪(1维)
```

这 7 个维度之间是**高度相关**的。例如：
*   要抓取桌上的杯子：`Δx`, `Δy`, `Δz` 必须协调指向杯子的位置。
*   要从侧面夹取：旋转 `Δroll` 和位移 `Δx` 必须配合。
*   夹爪闭合时机必须与到达位置同步。

这些维度之间的关联关系，用数学描述就是**联合分布 (Joint Distribution)**：

```
真实的动作分布：p(Δx, Δy, Δz, Δroll, Δpitch, Δyaw, gripper)
                 ↑ 所有维度的联合概率，维度间相互依赖
```

### (b) 自回归模型如何处理多维动作？

自回归模型用**链式法则 (Chain Rule)** 将联合分布分解为条件分布的乘积：

```
p(a₁, a₂, ..., a₇) = p(a₁) · p(a₂|a₁) · p(a₃|a₁,a₂) · ... · p(a₇|a₁,...,a₆)
```

在 RT-2 / OpenVLA 中，具体做法是：

```
步骤 1: 生成 Δx  的 Token  → p(Δx)
步骤 2: 生成 Δy  的 Token  → p(Δy | Δx)
步骤 3: 生成 Δz  的 Token  → p(Δz | Δx, Δy)
步骤 4: 生成 Δroll 的 Token → p(Δroll | Δx, Δy, Δz)
...
步骤 7: 生成 gripper Token  → p(gripper | Δx, ..., Δyaw)
```

**数学上，链式法则是精确的**——理论上它可以完美表示任何联合分布。那为什么说自回归模型"无法建模联合分布"呢？

### (c) 实际中的三大问题

#### 问题 1：离散化导致信息丢失

自回归模型将每个连续维度**离散化为 256 个 Bin**：

```
连续值: Δx = 0.03782 m
                ↓ 离散化（量化为 256 个桶之一）
离散 Token: Bin #137 (代表 [0.037, 0.039] 这个范围)
                ↓
量化误差: |0.03782 - 0.038| = 0.00018 m

精度：连续值范围 / 256 ≈ 0.4% 的分辨率
```

**问题**：256 Bin 对需要亚毫米精度的精细操作（如插钉入孔、缝纫）来说**精度不够**。增加到 1024 Bin？词表会爆炸，训练更困难。

#### 问题 2：维度间的条件建模不够好

虽然链式法则在数学上是精确的，但实际中 LLM 的**有限容量**导致条件概率的建模不够准确：

```
理论:  p(Δy | Δx) 可以表示 Δy 和 Δx 之间的任意复杂关系
实际:  Transformer 只通过前面几个 Token 的 Attention 来近似这种关系
       → 当维度间存在复杂的非线性耦合时，近似误差很大
```

**例子**：双臂操作中，左臂的 Δx_left 和右臂的 Δx_right 必须**精确对称**才能协作抬起物体。自回归模型先生成左臂动作、再生成右臂动作，右臂"试图"通过条件概率来配合左臂——但这种间接配合在实践中精度不足。

#### 问题 3：多模态分布（最关键的问题）

**多模态分布 (Multimodal Distribution)** = 同一情境下有多种同样正确的动作。

```
场景: "把杯子放到桌上" → 左边放 ✅ 右边放 ✅ 中间放 ✅

真实的动作分布 p(a) 有多个峰值（mode）：

       ┌─ 峰 1: 向左移动     峰 2: 向右移动 ─┐
概率   │     ╱╲                   ╱╲           │
  ↑    │    ╱  ╲                 ╱  ╲          │
  │    │   ╱    ╲     ╱╲       ╱    ╲         │
  │    │  ╱      ╲   ╱  ╲     ╱      ╲        │
  └────╱──────────╲─╱────╲───╱────────╲──→ Δx
       左           中间            右
```

**自回归模型的困境：Mode Averaging（模态平均）**

```
自回归模型逐步生成时：
  第 1 维 Δx：看到"左"和"右"两个模态
  → 取了中间值（"平均"了两个模态）
  → Δx ≈ 0（既不是左也不是右！）

结果：模型输出了一个"折中"但**不正确**的动作
      杯子既不放到左边也不放到右边，而是停在半空中 ❌
```

**数学解释：**

```
假设真实分布是双峰的：
  p(Δx) = 0.5 · N(Δx; -0.1, 0.01) + 0.5 · N(Δx; +0.1, 0.01)
                 ↑ 向左移动                    ↑ 向右移动

自回归模型倾向于输出期望值：
  E[Δx] = 0.5 × (-0.1) + 0.5 × (0.1) = 0  ← 模态平均！

→ 输出 Δx = 0 的概率最高，但这恰恰是最差的动作
```

**扩散模型不会有这个问题**——它通过随机采样从其中一个峰值生成，不会取平均：

```
扩散模型采样：
  第 1 次采样: Δx = -0.098 (从"左"模态采样) ✅
  第 2 次采样: Δx = +0.102 (从"右"模态采样) ✅
  → 每次都是有效动作，不会取折中值
```

### (d) 问题的严重程度

| 场景 | 是否有多模态分布？ | 自回归模型表现 |
| :--- | :--- | :--- |
| 简单抓取（唯一正确动作） | ❌ 单峰 | ✅ 没问题 |
| 物品放置（多个合理位置） | ✅ 多峰 | ⚠️ 可能折中 |
| 双臂协作（需精确配合） | ✅ 复杂耦合 | ❌ 配合不好 |
| 灵巧操作（高精度要求） | ✅ 连续高维 | ❌ 离散化精度不足 |
| 擦桌子（多种合理轨迹） | ✅ 强多模态 | ❌ 模态平均严重 |

---

## 3. 五大 VLA 架构范式详解

根据 2025 年的综合综述（覆盖 102 个 VLA 模型），VLA 架构可分为五大范式。

### (a) 自回归范式 (Autoregressive VLA)

#### 核心思想

把动作当成 Token，像生成文本一样逐个生成——**"Action as Language"**。

```
输入: [Image Tokens] [Text Tokens] → LLM → [Action Token 1] [Action Token 2] ... [Action Token 7]
```

#### 数学公式

动作离散化：将每个连续维度量化为 K 个 Bin（通常 K=256）：

```
离散化: â_i = Quantize(a_i, K) = round((a_i - a_min) / (a_max - a_min) × (K-1))

自回归生成:
  p(â₁, â₂, ..., â_D | o, l) = ∏ᵢ₌₁ᴰ p(âᵢ | â₁, ..., âᵢ₋₁, o, l)

  其中: o = 视觉观测, l = 语言指令, D = 动作维度数(如7)

训练损失 (Cross-Entropy):
  L = -∑ᵢ₌₁ᴰ log p(âᵢ* | â₁*, ..., âᵢ₋₁*, o, l)

  其中 âᵢ* 是人类演示中的真实动作离散值
```

#### 代表模型

*   **RT-2** (2023, Google)：55B PaLI-X/12B PaLM-E，首个 VLA。
*   **OpenVLA** (2024)：7B Llama 2 + DINOv2 + SigLIP，开源标杆。

#### 优点
*   架构最简单，**零额外参数**——直接复用 VLM 的自回归框架。
*   天然支持语言指令与动作的联合建模，继承 VLM 的**涌现能力**（语义推理、类比）。
*   训练稳定，与 VLM 预训练流程一致（Cross-Entropy Loss）。

#### 缺点
*   离散化精度有限（256 Bin ≈ 0.4% 精度），不适合亚毫米级操作。
*   逐 Token 串行生成，**推理速度慢**。
*   **多模态分布建模差**（Mode Averaging 问题，详见上方第 2 节）。
*   误差传播：前面维度的预测误差会影响后续维度。

---

### (b) 扩散范式 (Diffusion VLA)

#### 核心思想

LLM 输出条件特征，接一个扩散模型来**迭代去噪生成连续动作**。

```
输入: [Image] [Text] → VLM → [Hidden State h]
                                    ↓ 条件输入
                   [纯噪声 aT] → 🌊 去噪 → aT-1 → ... → a1 → a0 → [连续动作序列]
```

#### 数学公式

**前向扩散过程（训练时）**：给真实动作逐步加噪。

```
q(aₜ | aₜ₋₁) = N(aₜ; √(1-βₜ) aₜ₋₁, βₜI)

其中 βₜ 是噪声调度表(noise schedule), t = 1, ..., T

等价闭式: aₜ = √ᾱₜ · a₀ + √(1-ᾱₜ) · ε,   ε ~ N(0, I)
其中 ᾱₜ = ∏ₛ₌₁ᵗ (1 - βₛ)
```

**反向去噪过程（推理时）**：从纯噪声迭代恢复动作。

```
pθ(aₜ₋₁ | aₜ, h) = N(aₜ₋₁; μθ(aₜ, t, h), σₜ²I)

其中 h 是 VLM 的条件特征
μθ 由噪声预测网络 εθ(aₜ, t, h) 得到
```

**训练损失（Noise Prediction）**：

```
L = E_{a₀, ε, t} [ ||ε - εθ(aₜ, t, h)||² ]

即：预测加在动作上的噪声，与真实噪声的 MSE
```

**π0 使用的 Flow Matching（扩散的简化变体）**：

```
Flow Matching 定义一条从噪声 a₁ 到数据 a₀ 的直线路径：
  aₜ = (1-t) · a₀ + t · ε,     t ∈ [0, 1]

速度场: vθ(aₜ, t, h) 预测 "数据方向"
损失: L = E_{a₀, ε, t} [ ||vθ(aₜ, t, h) - (ε - a₀)||² ]

推理: 从 a₁ = ε (纯噪声) 出发，沿 vθ 积分到 t=0 得到动作
```

#### 代表模型

*   **Diffusion Policy** (2023, Columbia)：奠基工作，15 个任务平均 +46.9%。
*   **π0** (2024, Physical Intelligence)：Flow Matching + VLM，多机器人通用。
*   **Octo** (2024)：Transformer + Diffusion Head，开源通用策略。

#### 优点
*   输出**连续值**，精度极高，无离散化损失。
*   **天然建模多模态分布**——通过随机采样，每次推理可以从不同模态采样，不会取平均。
*   **Action Chunking**：一次并行去噪生成整段动作序列（如未来 16 步），提高平滑度。
*   训练目标简单（MSE 预测噪声）。

#### 缺点
*   去噪需要**多步迭代**（通常 10-100 步），推理速度远慢于单步前向。
*   扩散头通常是**独立模块**，与 VLM 骨干分离训练，可能损害知识迁移。
*   训练和调参复杂（噪声调度、去噪步数、条件注入方式等超参数多）。
*   不直接输出语言，多任务联合能力弱于自回归。

---

### (c) 强化学习范式 (RL-based VLA)

#### 核心思想

不依赖人工演示——用 VLM 作为**奖励模型**或**价值评估器**，配合 RL 让机器人**自主探索学习**。

```
方式 1: VLM-as-Reward
  [机器人执行动作] → [拍到的画面] → VLM 评估："你做得好不好？" → 奖励信号 r
                                                                    ↓
                                    RL 算法 (PPO/SAC) 用 r 更新策略 π

方式 2: VLA + RL Fine-Tuning
  预训练 VLA (模仿学习) → 用 RL 在线微调 → 持续优化策略
```

#### 数学公式

**RL-VLM-F（VLM 生成奖励）**：

```
给定任务描述 l 和两个观测状态 (o₁, o₂):
  VLM 给出偏好判断: VLM(o₁, o₂, l) → "o₂ 比 o₁ 更接近完成任务"

训练奖励模型 R(o, l):
  L_reward = -E[ log σ(R(o₂, l) - R(o₁, l)) ]    (Bradley-Terry 偏好模型)

RL 优化:
  max_π E[ ∑ₜ γᵗ R(oₜ, l) ]    (最大化累积奖励)
```

**VLA-RL（在线 RL 微调 VLA）**：

```
阶段 1: 预训练 VLA π₀ (模仿学习, Cross-Entropy Loss)
阶段 2: 用 VLM 生成过程奖励模型 (Process Reward Model)
阶段 3: 在线 RL 微调:
  L = L_policy(π, R) + λ · KL(π || π₀)

  其中 KL 约束防止策略偏离预训练知识太远
```

#### 代表方法

*   **RL-VLM-F** (2024, ICML)：VLM 自动标注偏好 → 训练奖励函数。
*   **VLAC** (2024)：InternVL 同时输出奖励 + 动作，单次评估达 ~90% 成功率。
*   **VLA-RL** (2025)：预训练 VLA + 在线 RL 微调，在 LIBERO 上提升 4.5%。
*   **World-VLA-Loop**：世界模型 + VLA 闭环学习。

#### 优点
*   **无需人工演示数据**——VLM 提供奖励信号，机器人自主探索。
*   可以优化**长期目标**（不只是模仿当前帧的动作，而是最大化长期成功率）。
*   通过在线学习持续**自我提升**，突破模仿学习的性能上限。
*   **自动奖励工程**——VLM 替代了传统 RL 中最痛苦的手动奖励设计。

#### 缺点
*   训练**不稳定**，RL 的方差大、收敛慢。
*   **样本效率低**——需要大量环境交互（或高质量仿真器）。
*   VLM 生成的奖励**可能有噪声或偏差**，导致策略学偏。
*   需要**安全机制**——自主探索可能导致危险动作。
*   基础设施复杂——需要仿真器或真实机器人在线交互。

---

### (d) 混合范式 (Hybrid VLA)

#### 核心思想

组合多种范式的优势，取长补短。最常见的组合是**自回归的语义推理 + 扩散/其他方法的精确动作生成**。

```
典型架构:
  [Image] [Text] → 自回归 VLM (语义理解) → 条件特征 h
                                                ↓
                          扩散/FAST/离散扩散 (精确动作) → [动作序列]
```

#### 三种代表性混合方式

**混合方式 1：自回归 VLM + 扩散动作头（π0）**

```
训练:
  L = L_VLM(语言+视觉, Cross-Entropy) + L_Diffusion(动作, Flow Matching)

推理:
  VLM 编码 [图像, 指令] → h → Flow Matching 去噪 → 连续动作 chunk
```

**混合方式 2：自回归框架 + FAST 频域编码（π0-FAST）**

```
FAST 编码: 将连续动作序列 → DCT(离散余弦变换) → 频域压缩 → 离散 Token
  a₁:T → DCT → [低频系数, 高频系数(截断)] → Quantize → Token 序列

→ 在自回归 LLM 中直接预测 FAST Token，兼得自回归的简洁和连续值精度
```

**混合方式 3：离散扩散 (Discrete Diffusion VLA)**

```
核心: 在单一 Transformer 中用离散扩散建模离散化的动作 chunk

前向过程: 真实动作 Token → 逐步替换为 [MASK] Token
反向过程: [MASK] Token → 逐步预测恢复为动作 Token

训练损失 (与 VLM 相同的 Cross-Entropy):
  L = -E_{t, mask} [ ∑ᵢ∈masked log p(âᵢ | visible tokens, o, l) ]

关键优势:
  1. 自适应解码顺序：先恢复"容易"的 Token，再恢复"难"的
  2. 二次重遮蔽 (Re-masking)：不确定的预测被重新遮蔽并再次预测
  3. 并行解码：一次恢复多个 Token，打破自回归的串行瓶颈
```

#### 代表模型

*   **π0** (2024, Physical Intelligence)：VLM + Flow Matching，多机器人通用。
*   **π0-FAST** (2024)：VLM + FAST 频域编码，推理速度快。
*   **Discrete Diffusion VLA** (2025, ICLR 2026)：统一离散扩散，LIBERO 96.3%。
*   **HybridVLA** (2025)：自回归 + 扩散的自适应融合。

#### 优点
*   **取长补短**——自回归的语义推理 + 扩散的精确动作。
*   Discrete Diffusion VLA 将函数评估次数降低 **4.7 倍**（比连续扩散快得多）。
*   FAST 编码在保持精度的同时让自回归框架处理连续动作。
*   可以在单一 Transformer 中统一训练（Discrete Diffusion VLA），保留预训练知识。

#### 缺点
*   架构**复杂度增加**——多个组件需要协调训练。
*   扩散头与 VLM 骨干可能互相干扰（需要梯度隔离等技巧）。
*   超参数更多（扩散步数、FAST 截断频率、混合比例等）。
*   部分方法（如 π0）需要**多 GPU 训练**，对硬件要求高。

---

### (e) 专用范式 (Specialized VLA)

#### 核心思想

为特定任务类型、机器人形态或应用场景**定制架构**，放弃通用性以换取专项性能。

```
通用 VLA:  [任意图像 + 任意指令] → 通用策略 → [通用动作]
专用 VLA:  [特定传感器 + 特定任务] → 定制策略 → [特定格式动作]
```

#### 代表方向

**导航专用 VLA**：

```
输入: [全景图/深度图] + "去厨房拿杯水"
                ↓
  VLM 理解指令 → 拓扑地图规划 → 导航路径点
                                    ↓
  低级运动控制器 → [线速度, 角速度]
```

*   输入特化：使用全景图/深度图/激光雷达，而非普通 RGB。
*   输出特化：导航只需 2D 速度指令（线速度+角速度），不需要 6DoF 机械臂动作。
*   代表：NaVILA、LM-Nav。

**双臂/灵巧手操作 VLA**：

```
输入: [多视角相机] + [两只手的力觉传感器] + 指令
                ↓
  VLM → 双流动作头: 左臂动作 + 右臂动作 (需要同步协调)
```

*   输出特化：动作空间翻倍（左臂 7DoF + 右臂 7DoF = 14DoF）。
*   需要显式的双臂协调机制，通用自回归模型很难处理。
*   代表：ALOHA 系列、π0（虽然也能做通用，但其双臂能力是定制优化的）。

**高频控制 VLA**：

```
要求: 控制频率 > 50 Hz (如无人机、高速工具操作)
                ↓
  轻量级模型 (< 1B) + 边缘推理优化 + Action Chunking
```

*   架构特化：极度轻量化，牺牲理解能力换取推理速度。
*   代表：EdgeVLA、LiteVLA。

#### 优点
*   **专项性能最强**——在其擅长领域通常超越通用模型。
*   可以针对传感器和动作空间**高度优化**。
*   推理效率可以极高（如导航 VLA 只输出 2 维速度）。

#### 缺点
*   **泛化性差**——换任务/换机器人可能需要从头设计。
*   无法享受"大一统模型"的涌现能力和跨任务迁移。
*   每种任务都需要专门的工程投入。

---

## 4. 五大范式全面对比

### (a) 核心特性对比

| 特性 | 自回归 | 扩散 | 强化学习 | 混合 | 专用 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **动作精度** | 中（离散 256 Bin） | **最高（连续值）** | 取决于底层策略 | 高 | 取决于设计 |
| **多模态分布** | ❌ Mode Averaging | **✅ 天然支持** | ✅ 可学习多模态 | ✅ | 取决于设计 |
| **推理速度** | 慢（串行 Token） | 慢（多步去噪） | 快（一次前向） | 中等 | **可以最快** |
| **训练难度** | **最低** | 中等 | **最高** | 高 | 中等 |
| **语义理解** | **最强** | 弱（分离的头） | 间接（通过奖励） | 强 | 取决于设计 |
| **需要演示数据** | ✅ | ✅ | **❌ 可自主探索** | ✅ | ✅ |
| **长期优化** | ❌ 模仿当前帧 | ❌ 模仿当前帧 | **✅ 优化长期回报** | ❌ | 取决于设计 |

### (b) 性能基准对比（LIBERO Benchmark）

| 方法 | 范式 | LIBERO 平均成功率 | 说明 |
| :--- | :--- | :--- | :--- |
| **Discrete Diffusion VLA** | 混合 | **96.3%** | 离散扩散 + Transformer |
| **π0-FAST** | 混合 | ~90%+ | Flow Matching + FAST |
| **π0** | 扩散 | ~88% | Flow Matching |
| **OpenVLA (微调)** | 自回归 | ~85% | Llama 2 7B |
| **Octo** | 扩散 | ~78% | Transformer + Diffusion |
| **VLA-RL (OpenVLA+RL)** | RL | ~89.5% | OpenVLA + 在线 RL |

### (c) 各范式擅长领域

| 范式 | 最擅长的任务 | 原因 | 不擅长的任务 |
| :--- | :--- | :--- | :--- |
| **自回归** | 需要**语义推理**的任务：理解指令中的隐含意义（"拿重的东西当锤子"）、跨物体泛化 | VLM 的知识和推理能力直接保留 | 需要高精度或多模态动作的灵巧操作 |
| **扩散** | 需要**精细控制**的任务：灵巧操作、插钉、缝纫、多种有效路径的擦拭 | 连续值精度高 + 多模态建模 | 需要语义推理或开放指令的任务 |
| **RL** | **长程任务**和没有演示数据的场景：自主学习整理房间、突破模仿学习上限 | 优化长期累积奖励 + 不需要演示 | 需要大量交互、且不能碰撞的安全敏感任务 |
| **混合** | **综合性任务**：既需要理解指令又需要精细操作的多步骤任务 | 结合了自回归的推理和扩散的精度 | 对延迟极敏感的实时控制（架构复杂推理慢） |
| **专用** | **特定垂直领域**：高频无人机控制、视觉导航、双臂叠衣服 | 为特定场景高度优化 | 跨任务泛化、开放世界场景 |

### (d) 如何选择架构？决策树

```
你的任务需要什么？
  │
  ├─ 需要理解复杂语言指令？(如 "把看起来最贵的东西放到保险箱")
  │   └─→ 自回归 VLA (RT-2, OpenVLA) 或 混合 (π0, HybridVLA)
  │
  ├─ 需要亚毫米精度？(如 插USB、装螺丝)
  │   └─→ 扩散 VLA (Diffusion Policy, π0) 或 混合 (Discrete Diffusion VLA)
  │
  ├─ 有多种等效正确动作？(如 擦桌子、叠衣服)
  │   └─→ 扩散 VLA (必须用扩散才能避免 Mode Averaging)
  │
  ├─ 没有人工演示数据？
  │   └─→ RL-based VLA (RL-VLM-F, VLA-RL)
  │
  ├─ 需要极低延迟实时控制？(如 无人机、高速抓取)
  │   └─→ 专用 VLA (EdgeVLA, LiteVLA)
  │
  └─ 什么都需要？
      └─→ 混合 VLA (Discrete Diffusion VLA, π0-FAST)
          这是当前的前沿方向，但架构最复杂
```

## 5. 视觉编码器的选择

| 编码器 | 预训练方式 | 用于 VLA 的代表 | 特点 |
| :--- | :--- | :--- | :--- |
| **CLIP ViT** | 对比学习 (图文对齐) | RT-2 | 语义丰富，但缺少空间细节 |
| **SigLIP** | Sigmoid 对比学习 | OpenVLA | 比 CLIP 训练更稳定 |
| **DINOv2** | 自监督学习 (MAE) | OpenVLA | 空间细节丰富，适合精细操作 |
| **DINOv2 + SigLIP** | 双编码器融合 | **OpenVLA** | 🔑 语义 + 空间 = 最佳组合 |
| **ViT (from scratch)** | 联合训练 | Octo | 灵活但需要更多数据 |

> **关键洞察**：OpenVLA 的 DINOv2 + SigLIP 双编码器策略成为了 VLA 视觉编码的标准做法——DINOv2 提供空间精度（物体在哪里），SigLIP 提供语义理解（物体是什么）。

## 6. 骨干 LLM 的角色

LLM 在 VLA 中不仅仅是"语言模型"，它扮演的是**多模态融合与推理引擎**：

| 功能 | 说明 |
| :--- | :--- |
| **多模态融合** | 将视觉 Token、语言 Token、本体感受 Token 在统一的注意力中交互 |
| **指令理解** | 解析自然语言指令的含义和意图 |
| **世界知识** | 利用预训练知识理解物体属性（"玻璃杯是易碎的"） |
| **推理** | 多步推理能力（"要打开抽屉，先要松开手里的东西"） |
| **时序建模** | 通过历史帧理解动作的时间依赖关系 |

常用 LLM 骨干：

| LLM | 参数量 | 用于 VLA |
| :--- | :--- | :--- |
| Llama 2 7B | 7B | OpenVLA |
| PaLI-X | 55B | RT-2 |
| PaLM-E | 12B | RT-2 |
| Qwen2 | 多种 | 最新 VLA 工作 |

## 7. 动作解码头的比较

| 动作头类型 | 输出方式 | 精度 | 速度 | 多模态分布 | 代表 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **分类头 (Binning)** | 每维 256 类 | 中 | 快 | ❌ | RT-1, RT-2 |
| **自回归 Token** | 逐 Token 生成 | 中 | 慢 | ❌ | OpenVLA |
| **MLP 回归** | 直接输出连续值 | 高 | 最快 | ❌ | 简单 BC |
| **扩散头 (Diffusion)** | 迭代去噪 | **最高** | 慢 | **✅** | π0, Octo |
| **FAST Token** | 频域压缩 Token | 高 | 快 | ❌ | π0-FAST |

## 8. 实际架构案例

### RT-2 架构

```
[Image 300x300] → ViT-G (4B) → 视觉 Token
                                    ↓
[Text Instruction] → Tokenizer → 文本 Token → PaLI-X (55B) → 自回归解码
                                                               ↓
                                              "1 128 91 241 5 101 128"
                                                               ↓
                                              解码为 7 维连续动作
```

### OpenVLA 架构

```
[Image 224x224] → DINOv2 + SigLIP (双编码器) → 融合视觉 Token
                                                      ↓
[Text Instruction] → Tokenizer → 文本 Token → Llama 2 (7B) → 自回归解码
                                                                ↓
                                                     7 个动作 Token (每维 256 Bin)
                                                                ↓
                                                      解码为 7 维连续动作
```

### π0 架构

```
[Multi-View Images] → ViT → 视觉 Token
                                ↓
[Text Instruction] → Tokenizer → 文本 Token → Pre-trained VLM → Hidden State
                                                                     ↓
[Proprioception] → MLP → 本体 Token ──────────────────────────→ Diffusion Head
                                                                     ↓
                                                          Action Chunk (未来 N 步动作)
```

## 9. 关键架构趋势

1.  **双视觉编码器成为标配**：DINOv2 (空间) + SigLIP/CLIP (语义) 的组合被广泛采用。
2.  **扩散头取代分类头**：精度更高，能建模多模态分布，适合灵巧操作。
3.  **Action Chunking (动作分块)**：一次预测未来 N 步动作（如 N=16），而非逐步预测，提高控制平滑度和推理效率。
4.  **多视角输入**：从单一腕部摄像头 → 腕部 + 全局两个摄像头 → 更多视角，提高空间感知。
5.  **MoE 架构渗透**：Qwen3-Omni 等 MoE 模型开始进入 VLA 领域，以较少的激活参数实现更大模型容量。

> **一句话总结**：VLA 架构 = VLM 架构 + 动作解码头。视觉编码器看世界，LLM 理解并推理，动作头将意图转化为物理动作——三者协同让 AI 从"看懂"走向"做到"。
